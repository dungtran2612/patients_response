{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version()) # check python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a696030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# library version\n",
    "print(pd.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361873",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with pandas\n",
    "data = pd.read_csv(input_dir +'pheno_met.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = data['Cortiso'].describe()\n",
    "\n",
    "# Display results\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e62c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Min_Cortisol_Group'] = data['Cortisol'] > 18\n",
    "data['Min_Cortisol_Group']\n",
    "data['Min_Cortisol_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d3af5",
   "metadata": {},
   "source": [
    "**M100022013: tetrahydrocortisol glucuronide; M100022127: tetrahydrocortisone glucuronide (5); M100000963: homocitrulline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094398be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Age_at_plasma_collection_date','Corticosteroids_total_number_of_prescriptions_within_5y_log',\n",
    "           'M100022013','M100022127','M100000963']  # this list is for symmetric numeric columns\n",
    "\n",
    "cat_cols = ['Gender_impute_all', 'Race_White_KNN_impute_missing', \n",
    "            'Closest_collect_date_smoking_status', \n",
    "            'BMI_median_closest_measure_date_to_collect_date_category',\n",
    "            'ICS_Dose_Classification_5Y_Median', \n",
    "            'Cortisol_closest_date_collect_date_gap_abs_quartile', \n",
    "            'Any_Bronchiectasis_Existence_Yes_No',\n",
    "            'Any_Chronic_Bronchitis_Existence_Yes_No']  # this list for the class columns\n",
    "\n",
    "target = 'Min_Cortisol_Group'    # this is the name of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_pipeline = Pipeline([                           # now we need a small pipeline for numeric columns since it has two steps\n",
    "    ('impute', SimpleImputer(strategy='median')),   # this step will impute missing values using column medians\n",
    "    ('standardize', StandardScaler())               # this step will scale all numeric columns\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "processing_pipeline = ColumnTransformer([      # this transformer merges the processed numeric columns and class columns\n",
    "    ('numeric', num_pipeline, num_cols),                                                       # numeric columns\n",
    "    ('class', OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False), cat_cols) #encoder to transform class columns to numeric, this will automatically handle missing data\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d319e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run models\n",
    "# import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#result list\n",
    "results = []\n",
    "\n",
    "#loop 10 runs with set seeds\n",
    "for seed in [111, 222, 333, 444, 555, 666, 777, 888, 999, 101]:\n",
    "    #set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    #train test split\n",
    "    train_data, test_data = train_test_split(data, test_size = 0.2)\n",
    "    train_processed = processing_pipeline.fit_transform(train_data)\n",
    "    n_features = train_processed.shape[1]\n",
    "    \n",
    "    #logistic\n",
    "    logistic = LogisticRegression(max_iter=10000)\n",
    "    param_grid = [{'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1 , 5, 10, 50, 100]}]\n",
    "    grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    logistic_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    logistic_pipeline.fit(train_data, train_data[target])\n",
    "    logistic_train_auc = logistic_pipeline['modeling'].best_score_\n",
    "    logistic_test_auc = logistic_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #SVM\n",
    "    svc = SVC()\n",
    "    param_grid = [{\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'kernel' : ['rbf'],\n",
    "        'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }]\n",
    "\n",
    "    grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    svc_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    svc_pipeline.fit(train_data[:2000], train_data[target][:2000])\n",
    "    svc_train_auc = svc_pipeline['modeling'].best_score_\n",
    "    svc_test_auc = svc_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #RF\n",
    "    param_grid = [{\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'min_samples_split' : [0.05, 0.1, 0.2],\n",
    "        'min_samples_leaf' : [0.05, 0.1, 0.2],\n",
    "        'n_estimators': [10, 20, 50, 100]\n",
    "    }]\n",
    "\n",
    "    forest = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(forest, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    forest_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    forest_pipeline.fit(train_data, train_data[target])\n",
    "    forest_train_auc = forest_pipeline['modeling'].best_score_\n",
    "    forest_test_auc = forest_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #GB (use same parameter grid with RF, so no needs to redefine)\n",
    "    gbc = GradientBoostingClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(gbc, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    gbc_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    gbc_pipeline.fit(train_data, train_data[target])\n",
    "    gbc_train_auc = gbc_pipeline['modeling'].best_score_\n",
    "    gbc_test_auc = gbc_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #MLP\n",
    "    param_grid = [{\n",
    "        'hidden_layer_sizes' : [[n_features // 2, n_features // 2],\n",
    "                                [n_features // 2, n_features // 2, n_features // 2],\n",
    "                                [n_features, n_features],\n",
    "                                [n_features, n_features, n_features],\n",
    "                                [n_features*2, n_features*2],\n",
    "                                [n_features*2, n_features*2, n_features*2]],\n",
    "        'alpha' : [0.001, 0.01, 0.1, 1, 10]                                    #regularization terms\n",
    "    }]\n",
    "\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    mlp_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    mlp_pipeline.fit(train_data, train_data[target])\n",
    "    mlp_train_auc = mlp_pipeline['modeling'].best_score_\n",
    "    mlp_test_auc = mlp_pipeline.score(test_data, test_data[target])\n",
    "    results.append([seed,\n",
    "                    logistic_train_auc,\n",
    "                    logistic_test_auc,\n",
    "                    svc_train_auc,\n",
    "                    svc_test_auc,\n",
    "                    forest_train_auc,\n",
    "                    forest_test_auc,\n",
    "                    gbc_train_auc,\n",
    "                    gbc_test_auc,\n",
    "                    mlp_train_auc,\n",
    "                    mlp_test_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ea3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['seed',\n",
    "                      'logistic_train_auc',\n",
    "                      'logistic_test_auc',\n",
    "                      'svc_train_auc',\n",
    "                      'svc_test_auc',\n",
    "                      'forest_train_auc',\n",
    "                      'forest_test_auc',\n",
    "                      'gbc_train_auc',\n",
    "                      'gbc_test_auc',\n",
    "                      'mlp_train_auc',\n",
    "                      'mlp_test_auc']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '...'\n",
    "results_df.to_csv(output_dir + 'Prediction_model_result_of_validated_sig_metabolites_and_other_variables.csv', \n",
    "                  index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b9f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Jupyter Conda Test Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
