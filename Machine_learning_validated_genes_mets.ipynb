{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9dbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with pandas\n",
    "data = pd.read_csv(input_dir + '/sig_genes_mets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cortisol_Group'] = data['Cortisol'] > 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cortisol_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a146b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_met_list = list(data.loc[:,'MAN1C1':'M100022127'].columns)\n",
    "len(gene_met_list) # 76: 19 mets and 57 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = gene_met_list  # this list is for symmetric numeric columns\n",
    "\n",
    "cat_cols = []  # this list for the class columns\n",
    "\n",
    "target = 'Cortisol_Group'    # this is the name of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_pipeline = Pipeline([                           # now we need a small pipeline for numeric columns since it has two steps\n",
    "    ('impute', SimpleImputer(strategy='median')),   # this step will impute missing values using column medians\n",
    "    ('standardize', StandardScaler())               # this step will scale all numeric columns\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "processing_pipeline = ColumnTransformer([      # this transformer merges the processed numeric columns and class columns\n",
    "    ('numeric', num_pipeline, num_cols),                                                       # numeric columns\n",
    "    ('class', OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False), cat_cols) #encoder to transform class columns to numeric, this will automatically handle missing data\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put target name in target variable\n",
    "target = 'Cortisol_Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f501453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop 100 runs with set seeds\n",
    "for seed in range(111,11101,111):\n",
    "#for seed in range(1,2):\n",
    "    #set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    #train test split\n",
    "    train_data, test_data = train_test_split(data, test_size = 0.3)\n",
    "    train_processed = processing_pipeline.fit_transform(train_data)\n",
    "    n_features = train_processed.shape[1]\n",
    "    \n",
    "    #logistic\n",
    "    logistic = LogisticRegression(max_iter=10000)\n",
    "    param_grid = [{'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1 , 5, 10, 50, 100]}]\n",
    "    grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    logistic_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    logistic_pipeline.fit(train_data, train_data[target])\n",
    "    logistic_train_auc = logistic_pipeline['modeling'].best_score_\n",
    "    logistic_test_auc = logistic_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #SVM\n",
    "    svc = SVC()\n",
    "    param_grid = [{\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'kernel' : ['rbf'],\n",
    "        'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }]\n",
    "\n",
    "    grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    svc_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    svc_pipeline.fit(train_data, train_data[target])\n",
    "    svc_train_auc = svc_pipeline['modeling'].best_score_\n",
    "    svc_test_auc = svc_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #RF\n",
    "    param_grid = [{\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split' : [0.05, 0.1, 0.2],\n",
    "        'min_samples_leaf' : [0.05, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100]\n",
    "    }]\n",
    "\n",
    "    forest = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(forest, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    forest_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    forest_pipeline.fit(train_data, train_data[target])\n",
    "    forest_train_auc = forest_pipeline['modeling'].best_score_\n",
    "    forest_test_auc = forest_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #GB (use same parameter grid with RF, so no needs to redefine)\n",
    "    gbc = GradientBoostingClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(gbc, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    gbc_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    gbc_pipeline.fit(train_data, train_data[target])\n",
    "    gbc_train_auc = gbc_pipeline['modeling'].best_score_\n",
    "    gbc_test_auc = gbc_pipeline.score(test_data, test_data[target])\n",
    "\n",
    "    #MLP\n",
    "    param_grid = [{\n",
    "        'hidden_layer_sizes' : [[n_features // 2, n_features // 2],\n",
    "                                [n_features // 2, n_features // 2, n_features // 2],\n",
    "                                [n_features, n_features],\n",
    "                                [n_features, n_features, n_features],\n",
    "                                [n_features*2, n_features*2],\n",
    "                                [n_features*2, n_features*2, n_features*2]],\n",
    "        'alpha' : [0.001, 0.01, 0.1, 1, 10]                                    #regularization terms\n",
    "    }]\n",
    "\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "\n",
    "    mlp_pipeline = Pipeline([\n",
    "        ('processing', processing_pipeline),\n",
    "        ('modeling', grid_search)\n",
    "    ])\n",
    "\n",
    "    mlp_pipeline.fit(train_data, train_data[target])\n",
    "    mlp_train_auc = mlp_pipeline['modeling'].best_score_\n",
    "    mlp_test_auc = mlp_pipeline.score(test_data, test_data[target])\n",
    "    results.append([seed, \n",
    "                    logistic_train_auc,\n",
    "                    logistic_test_auc,\n",
    "                    svc_train_auc,\n",
    "                    svc_test_auc,\n",
    "                    forest_train_auc,\n",
    "                    forest_test_auc,\n",
    "                    gbc_train_auc,\n",
    "                    gbc_test_auc,\n",
    "                    mlp_train_auc,\n",
    "                    mlp_test_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b68d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['seed', \n",
    "                      'logistic_train_auc',\n",
    "                      'logistic_test_auc',\n",
    "                      'svc_train_auc',\n",
    "                      'svc_test_auc',\n",
    "                      'forest_train_auc',\n",
    "                      'forest_test_auc',\n",
    "                      'gbc_train_auc',\n",
    "                      'gbc_test_auc',\n",
    "                      'mlp_train_auc',\n",
    "                      'mlp_test_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b91a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '...'\n",
    "results_df.to_csv(output_dir + 'validated_genes_mets_machine_learning_result.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bef87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Conda Test Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
